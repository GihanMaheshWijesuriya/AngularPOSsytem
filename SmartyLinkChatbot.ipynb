{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SmartyLinkChatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7f11fwKS5oM0/2eOreSxb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GihanMaheshWijesuriya/AngularPOSsytem/blob/productionBranch/SmartyLinkChatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrJkoy_oEwM0",
        "outputId": "7d90b369-f547-4af8-e1f5-f9a7fb9db8e7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('popular')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import pickle\n",
        "import numpy as np"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzKglisDFVtV"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('model.h5')\n",
        "import json\n",
        "import random\n",
        "intents = json.loads(open('/content/sample_data/smartyLinkBotData.json').read())\n",
        "words = pickle.load(open('/content/texts.pkl', 'rb'))\n",
        "classes = pickle.load(open('/content/labels.pkl', 'rb'))"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZgPAT5IKGsE"
      },
      "source": [
        "def clean_up_sentence(sentence):\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "\n",
        "  sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "  return sentence_words"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu1X8WV1Kotr"
      },
      "source": [
        "def bow(sentence, words, show_details=True):\n",
        "\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "\n",
        "  bag = [0]*len(words)\n",
        "  for s in sentence_words:\n",
        "    for i,w in enumerate(words):\n",
        "      if w == s:\n",
        "        bag[i] = 1\n",
        "        if show_details:\n",
        "          print(\"found in bag: %s\" % w)\n",
        "  return(np.array(bag))"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCIr-4TNLZC7"
      },
      "source": [
        "def predic_class(sentence, model):\n",
        "\n",
        "  p = bow(sentence, words, show_details=False)\n",
        "  res = model.predict(np.array([p]))[0]\n",
        "  ERROR_THRESHOLD = 0.25\n",
        "  results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "\n",
        "  results.sort(key=lambda x:x[1], reverse=True)\n",
        "  result_list = []\n",
        "  for r in results:\n",
        "    result_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "  return result_list"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVDG69b5Y0SS"
      },
      "source": [
        "def getResponse(ints, intents_json):\n",
        "  tag = ints[0]['intent']\n",
        "  list_of_intents = intents_json['intents']\n",
        "  for i in list_of_intents:\n",
        "    if(i['tag'] == tag):\n",
        "      result = random.choice(i['responses'])\n",
        "      break\n",
        "  return result\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLQH8CffNE_k"
      },
      "source": [
        "def chatbot_response(msg):\n",
        "  ints = predic_class(msg, model)\n",
        "  res = getResponse(ints, intents)\n",
        "  return res"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5RhneRKOcka"
      },
      "source": [
        "from flask import Flask, render_template, request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app) \n",
        "\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qiB5d-cO6ws"
      },
      "source": [
        "@app.route('/')\n",
        "def home():\n",
        "    return 'Hello, World!'"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxVYrEzIPaAL"
      },
      "source": [
        "@app.route(\"/get\")\n",
        "def get_bot_response():\n",
        "  userText = request.args.get('msg')\n",
        "  return chatbot_response(userText)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbZKC6zhPqqH",
        "outputId": "38d6dade-0917-4dfb-e133-a4e6696b7d4e"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  app.run()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://34990f785247.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [12/May/2021 07:08:06] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [12/May/2021 07:08:07] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [12/May/2021 07:08:11] \"\u001b[37mGET /get?msg=hello HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [12/May/2021 07:08:28] \"\u001b[37mGET /get?msg=hello HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [12/May/2021 07:08:31] \"\u001b[37mGET /get?msg=hello HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [12/May/2021 07:08:41] \"\u001b[37mGET /get?msg=hello HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}